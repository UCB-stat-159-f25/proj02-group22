{"version":"1","records":[{"hierarchy":{"lvl1":"proj02"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"proj02"},"content":"","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"proj02"},"type":"lvl1","url":"/#proj02","position":2},{"hierarchy":{"lvl1":"proj02"},"content":"Template repository for Project 2, Stat 159/259 Fall 2025\n\nThe repository contain an environment.yml file to create the sotu environment, a data folder with the csv file, and outputs folder with the figures rendered in the notebook. The goal of this project is to perform text analysis on State of the Union speeches. The State of the Union speeches dataset for this project was obtained from Kaggle: \n\nhttps://â€‹wwwâ€‹.kaggleâ€‹.comâ€‹/datasetsâ€‹/nicholasheyerdahlâ€‹/stateâ€‹-ofâ€‹-theâ€‹-unionâ€‹-addressâ€‹-textsâ€‹-1790â€‹-2024â€‹?resourceâ€‹=â€‹download\n\nPart 1 performs exploratory data analysis on the number of speeches and word count for speeches given by each president over the years.\nPart 2 performs text processing using spaCy to extract and compare the most common tokens and lemmas.\nPart 3 uses a LDA model and BERTopic model to output the top words for each topic and topic distribution for the first speech.\nPart 4 uses implementation an analysis of word frequency over time using normalization, visializations, and keyword tracking.\n\n","type":"content","url":"/#proj02","position":3},{"hierarchy":{"lvl1":"Contribution statement"},"type":"lvl1","url":"/contribution-statement","position":0},{"hierarchy":{"lvl1":"Contribution statement"},"content":"","type":"content","url":"/contribution-statement","position":1},{"hierarchy":{"lvl1":"Contribution statement"},"type":"lvl1","url":"/contribution-statement#contribution-statement","position":2},{"hierarchy":{"lvl1":"Contribution statement"},"content":"Below is a summary of the contributions from each team member.","type":"content","url":"/contribution-statement#contribution-statement","position":3},{"hierarchy":{"lvl1":"Tse, Collin"},"type":"lvl1","url":"/contribution-statement#tse-collin","position":4},{"hierarchy":{"lvl1":"Tse, Collin"},"content":"*  Part 3\n*  \n*  ","type":"content","url":"/contribution-statement#tse-collin","position":5},{"hierarchy":{"lvl1":"Ke, Jacky"},"type":"lvl1","url":"/contribution-statement#ke-jacky","position":6},{"hierarchy":{"lvl1":"Ke, Jacky"},"content":"*  Part 4\n*  Part 5\n*","type":"content","url":"/contribution-statement#ke-jacky","position":7},{"hierarchy":{"lvl1":"Bachtra, Rebecca"},"type":"lvl1","url":"/contribution-statement#bachtra-rebecca","position":8},{"hierarchy":{"lvl1":"Bachtra, Rebecca"},"content":"* Part 2\n*\n*","type":"content","url":"/contribution-statement#bachtra-rebecca","position":9},{"hierarchy":{"lvl1":"Yau, Christy"},"type":"lvl1","url":"/contribution-statement#yau-christy","position":10},{"hierarchy":{"lvl1":"Yau, Christy"},"content":"*  Part 1\n*\n*\n\nSigned,\n\nTse, Collin\nKe, Jacky\nBachtra, Rebecca\nYau, Christy","type":"content","url":"/contribution-statement#yau-christy","position":11},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing"},"type":"lvl1","url":"/nlp-p01","position":0},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing"},"content":"","type":"content","url":"/nlp-p01","position":1},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"type":"lvl2","url":"/nlp-p01#part-1-data-loading-and-initial-exploration-15-pts","position":2},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"content":"The state of union speech data for this project is stored in the data folder in the SOTU.csv file. The data file is structured as a CSV with columns for president name, speech text, year, and word count in the speech.\n\nWe create the conda environment with the provided environment.yaml file.\n\n","type":"content","url":"/nlp-p01#part-1-data-loading-and-initial-exploration-15-pts","position":3},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"Read Data","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"type":"lvl3","url":"/nlp-p01#read-data","position":4},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"Read Data","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"content":"\n\n# imports\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('seaborn-v0_8-dark') \n\n# read in SOTU.csv \nsou = pd.read_csv('data/SOTU.csv')\nsou.head()\n\n","type":"content","url":"/nlp-p01#read-data","position":5},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"Exploratory Data Analysis","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"type":"lvl3","url":"/nlp-p01#exploratory-data-analysis","position":6},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"Exploratory Data Analysis","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"content":"We start with EDA to summarize the dataset and determine if any patterns or relationships exists between the variables. The mention of speeches refers to the State of the Union speeches. The five plots we create are:\n\nNumber of Speeches per President\n\nNumber of Speeches per Year\n\nDistribution of Word Count\n\nDistribution of Word Count over the years\n\nDistribution of Word Count per President\n\n","type":"content","url":"/nlp-p01#exploratory-data-analysis","position":7},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Number of Speeches per President","lvl3":"Exploratory Data Analysis","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"type":"lvl4","url":"/nlp-p01#number-of-speeches-per-president","position":8},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Number of Speeches per President","lvl3":"Exploratory Data Analysis","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"content":"\n\n# find number of speeches per president \nspeech_pres = sou['President'].value_counts().reindex(sou[\"President\"].unique())\n\n#bar plot \nfig, ax = plt.subplots(figsize=(12, 5))\nspeech_pres.plot(kind = 'bar')\nplt.title('Number of Speeches Per President')\nplt.xlabel('President')\nplt.ylabel('Count');\nplt.savefig('outputs/num_speeches_per_pres.png', dpi=300, bbox_inches='tight')\n\nWe created a bar plot for the number of speeches given by each president from 1970-2024. There are 43 presidents in the data set, so not all presidents who served during 1970-2024 are included. The number of speeches range from 1 to 12, with the majority of presidents giving 4 speeches and a few giving more than 8 speeches. State of the Union speeches are given annually, so we expect that most presidents would give 4 or 8 speeches (4 speeches per term). George Washington gave the most speeches (12 speeches) and Zachary Taylor gave the least (1 speech). George Washington served as president for 8 years, so it is suprising that he gave more than 8 speeches. On the other hand, Zachary Taylor was only president for a year so it makes sense that he only give one speech. The president who had the longest presidential term was Franklin D. Roosevelt, who served for 11 years and has the second-highest speech count (11 speeches).\n\n","type":"content","url":"/nlp-p01#number-of-speeches-per-president","position":9},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Number of Speeches per Year","lvl3":"Exploratory Data Analysis","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"type":"lvl4","url":"/nlp-p01#number-of-speeches-per-year","position":10},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Number of Speeches per Year","lvl3":"Exploratory Data Analysis","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"content":"\n\n# speeches per year\nspeech_year = sou['Year'].value_counts().sort_index()\n\n#line plot\nspeech_year.plot()\nplt.title('Number of State of Union Speeches Per Year')\nplt.xlabel('Year')\nplt.ylabel('Count');\nplt.savefig('outputs/num_speeches_per_year.png', dpi=300, bbox_inches='tight')\n\nWe created a lineplot of number of speeches per year. The years in the dataset range from 1790 to 2024 and the count of speeches per year ranges from 1 to 4. The majority of the years had 1 speech, which was expected since State of the Union speeches are given annually. The greatest number of speeches was 4 in 1790, which was during George Washgintonâ€™s term. This accounts for why Washington had 12 speeches despite only serving two terms. A few years have 2 speeches, which could correspond to presidents that had more than 8 speeches.\n\n","type":"content","url":"/nlp-p01#number-of-speeches-per-year","position":11},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Word Count Distribution","lvl3":"Exploratory Data Analysis","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"type":"lvl4","url":"/nlp-p01#word-count-distribution","position":12},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Word Count Distribution","lvl3":"Exploratory Data Analysis","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"content":"\n\n#histogram of word count\nsns.histplot(sou['Word Count'], bins = 18)\nplt.title('Distribution of State of the Union Speech Word Counts');\nplt.savefig('outputs/speech_word_count.png', dpi=300, bbox_inches='tight')\n\nWe created a histogram of the distribution of word counts in speeches. The distribution is right skewed, with few speeches (less than 10 speeches) having more than 15,000 words. The majority of speeches have around 3,400 to 6,700 words. The plot shows that there is a gap on the right end that is followed by one speech with more than 30,000 words, which can be an outlier.\n\n","type":"content","url":"/nlp-p01#word-count-distribution","position":13},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Word Count Distribution over Year","lvl3":"Exploratory Data Analysis","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"type":"lvl4","url":"/nlp-p01#word-count-distribution-over-year","position":14},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Word Count Distribution over Year","lvl3":"Exploratory Data Analysis","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"content":"\n\n# rugplot of word count distribution per year\nsns.scatterplot(x = sou['Word Count'], y = sou['Year'])\nsns.rugplot(x = sou['Word Count'], y = sou['Year'])\nplt.title('Speech Year Versus Word Count');\nplt.savefig('outputs/year_vs_word_count.png', dpi=300, bbox_inches='tight')\n\nWe created a rugplot for the distribution of word count and year. The plot also shows a scatterplot of word count per year (note that the y-axis is Year and x-axis is word count). The rugplot for word counts shows thicker bars for lower word counts, which represents a higher density, indicating a right skew like in the previous histogram of word count distribution. The scatterplot also shows some overplotting for points in the range of these lower word counts, indicating that most years had speeches with a word count less than 15,000. There are a couple points with a word count greater than 15,000.\n\n","type":"content","url":"/nlp-p01#word-count-distribution-over-year","position":15},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Word Count Distribution per President","lvl3":"Exploratory Data Analysis","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"type":"lvl4","url":"/nlp-p01#word-count-distribution-per-president","position":16},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl4":"Word Count Distribution per President","lvl3":"Exploratory Data Analysis","lvl2":"Part 1: Data Loading and Initial Exploration (15 pts)"},"content":"\n\n# mean word count per president\nword_count_pres = sou.groupby('President')['Word Count'].mean().reindex(sou[\"President\"].unique())\n\n#bar plot\nfig, ax = plt.subplots(figsize=(12, 5))\nword_count_pres.plot(kind = 'bar')\nplt.title('Average State of the Union Word Count per President')\nplt.ylabel('Average Word Count');\nplt.savefig('outputs/word_count_per_pres.png', dpi=300, bbox_inches='tight')\n\nWe found the average word count per president and created a bar plot. Each bar represents the average number of words in a State of the Union speech given by the president. The majority of presidents had less than 10,000 average words. William Howard Taft had the greatest average word count at more than 20,000 words, while John Adams had the lowest average word count. Notice that although George Washington gave the most speeches, he had one of the lowest average word counts in his speeches. The average word count was lowest for the first few presidents, which are located at the right end of the plot. Research shows that the first few presidents tended to give short speeches since they were personally delivered (spoken rather than delivered as a written report). Thomas Jefferson started a tradition of writing down the speech in the 19th century until personal delivery was revived by Woodrow Wilson. This accounts for the increase in average word count for presidents after Thomas Jefferson and a sudden decrease for Woodrow Wilson and subsequent presidents.","type":"content","url":"/nlp-p01#word-count-distribution-per-president","position":17},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization"},"type":"lvl1","url":"/nlp-p02","position":0},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization"},"content":"","type":"content","url":"/nlp-p02","position":1},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization","lvl3":"Read Data"},"type":"lvl3","url":"/nlp-p02#read-data","position":2},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization","lvl3":"Read Data"},"content":"\n\n# imports\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('seaborn-v0_8-dark') \n\n# read in SOTU.csv using pandas, name the variable `sou` for simplicity\n# the below cell is what the output should look like\n# read in SOTU.csv \nsou = pd.read_csv('data/SOTU.csv')\nsou.head()\n\n","type":"content","url":"/nlp-p02#read-data","position":3},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"type":"lvl2","url":"/nlp-p02#part-2-simple-text-processing-tokenization-lemmatization-word-frequency-vectorization-20-pts","position":4},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"content":"Now we will start working on simple text processing using the SpaCy package and the same dataset as Part 1. The package should already be included in the environment.yml. However, we will also need to download en_core_web_sm, an English language text processing model. To do this, while having your sotu environment activated, run the following:python -m spacy download en_core_web_sm\n\nNow, you should be good to go!\n\nSome important definitions:\n\nToken: a single word or piece of a word\n\nLemma: the core component of a word, e.g., â€œcompleteâ€ is the lemma for â€œcompletedâ€ and â€œcompletelyâ€\n\nStop Word: a common word that does not add semantic value, such as â€œaâ€, â€œandâ€, â€œtheâ€, etc.\n\nVectorization: representing a document as a vector where each index in the vector corresponds to a token or word and each entry is the count.\n\nIn this section, we will explore the most common tokens and lemmas throughout different slices of the speech data. We will also develop vectorization representations of the speeches.\n\nThe core steps are:\n\nProcess speeches using the SpaCy nlp module\n\nAnalyze Tokens vs Lemmas:\n\nCreate a list of all tokens across all speeches that are not stop words, punctuation, or spaces.\n\nCreate a second list of the lemmas for these same tokens.\n\nDisplay the top 25 for each of these and compare.\n\nAnalyze common word distributions over different years:\n\nCreate a function that takes the dataset and a year as an input and outputs the top n lemmas for that yearâ€™s speeches\n\nCompare the top 10 words for 2023 versus 2019\n\nDocument Vectorization:\n\nTrain a Term Frequency-Inverse Document Frequency (TF-IDF) vectorization model using your processed dataset and scikit learn\n\nOutput the feature vectors\n\nHelpful Resources:\n\nhttps://â€‹realpythonâ€‹.comâ€‹/naturalâ€‹-languageâ€‹-processingâ€‹-spacyâ€‹-python/\n\nhttps://â€‹wwwâ€‹.statologyâ€‹.orgâ€‹/textâ€‹-preprocessingâ€‹-featureâ€‹-engineeringâ€‹-spacy/\n\nhttps://â€‹scikitâ€‹-learnâ€‹.orgâ€‹/stableâ€‹/modulesâ€‹/generatedâ€‹/sklearnâ€‹.featureâ€‹_extractionâ€‹.textâ€‹.TfidfVectorizerâ€‹.html#\n\nhttps://â€‹wwwâ€‹.geeksforgeeksâ€‹.orgâ€‹/nlpâ€‹/howâ€‹-toâ€‹-storeâ€‹-aâ€‹-tfidfvectorizerâ€‹-forâ€‹-futureâ€‹-useâ€‹-inâ€‹-scikitâ€‹-learn/\n\n","type":"content","url":"/nlp-p02#part-2-simple-text-processing-tokenization-lemmatization-word-frequency-vectorization-20-pts","position":5},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization","lvl3":"Processing Speeches with SpaCy","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"type":"lvl3","url":"/nlp-p02#processing-speeches-with-spacy","position":6},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization","lvl3":"Processing Speeches with SpaCy","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"content":"Lets study just speeches from 2000 and onwards to begin with. So, be sure to subset your DataFrame to just these speeches before continuing!\n\n!pip install spacy\n!python -m spacy download en_core_web_sm\n\nimport spacy\nfrom tqdm import tqdm\nfrom collections import Counter\n\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Load the SOTU dataset\nsotu = pd.read_csv('data/SOTU.csv')\n\nprint(f\"\\nDataset loaded successfully!\")\nprint(f\"Shape: {sotu.shape}\")\nprint(f\"Columns: {sotu.columns.tolist()}\")\nprint(f\"Total speeches: {len(sotu)}\")\nprint(f\"Year range: {sotu['Year'].min()} - {sotu['Year'].max()}\")\n\n# subset the speech dataframe for speeches from 2000 and onwards\nprint(\"SUBSETTING DATA (2000 ONWARDS)\")\n\nsotu_2000 = sotu[sotu['Year'] >= 2000].copy()\n\nprint(f\"\\nFiltered dataset for analysis:\")\nprint(f\"  Speeches from 2000+: {len(sotu_2000)}\")\nprint(f\"  Year range: {sotu_2000['Year'].min()} - {sotu_2000['Year'].max()}\")\n\n\nprint(\"PROCESSING SPEECHES WITH SPACY (2000+ only)\")\n\n# Process speeches from 2000+ using nlp.pipe() for efficiency\nprocessed_docs = []\n\nfor doc in tqdm(nlp.pipe(sotu_2000['Text'].tolist(), batch_size=50), \n                total=len(sotu_2000), \n                desc=\"Processing speeches\"):\n    processed_docs.append(doc)\n\n","type":"content","url":"/nlp-p02#processing-speeches-with-spacy","position":7},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization","lvl3":"Analyze Tokens vs Lemmas","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"type":"lvl3","url":"/nlp-p02#analyze-tokens-vs-lemmas","position":8},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization","lvl3":"Analyze Tokens vs Lemmas","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"content":"","type":"content","url":"/nlp-p02#analyze-tokens-vs-lemmas","position":9},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization","lvl4":"Token List","lvl3":"Analyze Tokens vs Lemmas","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"type":"lvl4","url":"/nlp-p02#token-list","position":10},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization","lvl4":"Token List","lvl3":"Analyze Tokens vs Lemmas","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"content":"Create a list of tokens across all speeches that are not spaces, stopwords, or punctuation. Make each token lowercase as well. Hint: each element of the list we just created are themselves lists of tokens. Token objects have attributes is_stop, is_punct, and is_space.\n\nprint(\"ANALYZE TOKENS VS LEMMAS\")\n\n# TOKEN LIST\n\nprint(\"\\n--- Creating Token List ---\")\n\nall_tokens = []\n\nfor doc in processed_docs:\n    for token in doc:\n        if not token.is_stop and not token.is_punct and not token.is_space:\n            all_tokens.append(token.text.lower())\n\ntoken_counts = Counter(all_tokens)\ntop_25_tokens = token_counts.most_common(20)\ntop_25_tokens\n\n","type":"content","url":"/nlp-p02#token-list","position":11},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization","lvl4":"Lemma List","lvl3":"Analyze Tokens vs Lemmas","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"type":"lvl4","url":"/nlp-p02#lemma-list","position":12},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization","lvl4":"Lemma List","lvl3":"Analyze Tokens vs Lemmas","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"content":"Do the same as above, but for lemmas. Hint: recall lemmas are components of words. Each token should have an attribute to extract the lemma.\n\nall_lemmas = []\n\nfor doc in processed_docs:\n    for token in doc:\n        if not token.is_stop and not token.is_punct and not token.is_space:\n            all_lemmas.append(token.lemma_.lower())\n\nlemma_counts = Counter(all_lemmas)\ntop_25_lemmas = lemma_counts.most_common(25)\ntop_25_lemmas\n\n","type":"content","url":"/nlp-p02#lemma-list","position":13},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization","lvl4":"Token versus Lemma Comparison","lvl3":"Analyze Tokens vs Lemmas","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"type":"lvl4","url":"/nlp-p02#token-versus-lemma-comparison","position":14},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization","lvl4":"Token versus Lemma Comparison","lvl3":"Analyze Tokens vs Lemmas","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"content":"What do you notice about the top tokens versus the top lemmas?\nConsider two tokens - â€œyearâ€ and â€œyearsâ€ - how do their counts compare to the lemma â€œyearâ€?\nWhat about the lemma â€œchildâ€?\n\nprint(\"\\n--- Token vs Lemma Comparison ---\")\n\n# Compare \"year\" and \"years\"\nyear_count = token_counts.get('year', 0)\nyears_count = token_counts.get('years', 0)\nyear_lemma_count = lemma_counts.get('year', 0)\n\nprint(f\"\\nComparison for 'year'/'years':\")\nprint(f\"  Token 'year': {year_count}\")\nprint(f\"  Token 'years': {years_count}\")\nprint(f\"  Lemma 'year': {year_lemma_count}\")\n\n# Compare \"child\" and \"children\"\nchild_count = token_counts.get('child', 0)\nchildren_count = token_counts.get('children', 0)\nchild_lemma_count = lemma_counts.get('child', 0)\n\nprint(f\"\\nComparison for 'child'/'children':\")\nprint(f\"  Token 'child': {child_count}\")\nprint(f\"  Token 'children': {children_count}\")\nprint(f\"  Lemma 'child': {child_lemma_count}\")\n\n\n","type":"content","url":"/nlp-p02#token-versus-lemma-comparison","position":15},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization","lvl3":"Common Words","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"type":"lvl3","url":"/nlp-p02#common-words","position":16},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization","lvl3":"Common Words","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"content":"","type":"content","url":"/nlp-p02#common-words","position":17},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization","lvl4":"Common Words per Year Function","lvl3":"Common Words","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"type":"lvl4","url":"/nlp-p02#common-words-per-year-function","position":18},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization","lvl4":"Common Words per Year Function","lvl3":"Common Words","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"content":"Fill in the below function to obtain the n-most common words in speeches for a given year.\n\ninputs:\n\ndf raw unprocessed sou dataframe\n\nyear\n\nn\noutputs:\n\ntop n words for that years\n\nsteps:\n\nsubset the dataframe for the year of interest - note the years might not be in int type\n\nprocess the subsetted dataframe with spacy\n\nget the lemmas across all those speeches\n\ncount the top n lemmas\n\nprint(\"COMMON WORDS ANALYSIS\")\n\ndef get_most_common_words(df, year, n=25):\n    \"\"\"\n    Get the n most common lemmas for a given year.\n    \n    Parameters:\n    -----------\n    df : DataFrame\n        SOTU dataframe with 'Year' and 'Text' columns\n    year : int or float\n        Year of interest\n    n : int\n        Number of top words to return\n        \n    Returns:\n    --------\n    list of tuples: (lemma, count) for top n words\n    \"\"\"\n    # Subset for the year\n    df_year = df[df['Year'] == year]\n    \n    if len(df_year) == 0:\n        print(f\"No speeches found for year {year}\")\n        return []\n    \n    # Process and extract lemmas\n    year_lemmas = []\n    for doc in nlp.pipe(df_year['Text'].tolist()):\n        for token in doc:\n            if not token.is_stop and not token.is_punct and not token.is_space:\n                year_lemmas.append(token.lemma_.lower())\n    \n    # Count and return top n\n    return Counter(year_lemmas).most_common(n)\n\n# test it on 2024\nwords_2024 = get_most_common_words(sotu, 2024, n=10)\nprint(\"\\nTop 10 words for 2024:\")\nprint(words_2024)\n\n","type":"content","url":"/nlp-p02#common-words-per-year-function","position":19},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization","lvl4":"Compare 2023 to 2017","lvl3":"Common Words","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"type":"lvl4","url":"/nlp-p02#compare-2023-to-2017","position":20},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization","lvl4":"Compare 2023 to 2017","lvl3":"Common Words","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"content":"Run your function from the previous step to get the top 20 words for 2017 and 2023. Plot the words and their frequencies in a barchart and replicate the figure below.\n\nwords_2023 = get_most_common_words(sotu, 2023, n=20)\nwords_2017 = get_most_common_words(sotu, 2017, n=20)\n\nwords_2023\n\nwords_2017\n\n# Hint - put the words and counts into a pd Dataframe for better structure\n# and to make plotting easier\ndf_2017 = pd.DataFrame(words_2017, columns=[\"Word\", \"Count\"])\ndf_2023 = pd.DataFrame(words_2023, columns=[\"Word\", \"Count\"])\n\n# Draw the bar charts\nfig, axes = plt.subplots(2, 1, figsize=(12, 8))\n\n# Chart for 2017\nsns.barplot(data=df_2017, x=\"Word\", y=\"Count\", ax=axes[0])\naxes[0].set_title(\"Most Frequent Terms in the 2017 Address\")\naxes[0].tick_params(axis=\"x\", rotation=45)\n\n# Chart for 2023\nsns.barplot(data=df_2023, x=\"Word\", y=\"Count\", ax=axes[1])\naxes[1].set_title(\"Most Frequent Terms in the 2023 Address\")\naxes[1].tick_params(axis=\"x\", rotation=45)\n\nplt.subplots_adjust(hspace=0.5)\nplt.tight_layout()\nplt.savefig(\"outputs/2.1_State_of_the_Union_Most_Frequent_Words.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n","type":"content","url":"/nlp-p02#compare-2023-to-2017","position":21},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization","lvl3":"TF-IDF Vectorization","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"type":"lvl3","url":"/nlp-p02#tf-idf-vectorization","position":22},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization","lvl3":"TF-IDF Vectorization","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"content":"To use statsitical alorithms on documents, we need to transform them into vectors, where each element of the vector corresponds to a particular word in a document or corpus of documents. One common way is via TF-IDF embeddings. LLMs work similarly - they typically use transformer models to generate text embeddings before sending text through a deep neural network.\n\nHere we will fit a TF-IDF vectorizer, plot all the speeches on a 2-D grid using PCA and also using a heatmap, and examine TF-IDF scores for the top 10 most common words in the first speech. This is a good resource here: \n\nhttps://â€‹mediumâ€‹.comâ€‹/GeoffreyGordonAshbrookâ€‹/vectorâ€‹-visualizationâ€‹-2dâ€‹-plotâ€‹-yourâ€‹-tfâ€‹-idfâ€‹-withâ€‹-pcaâ€‹-83fa9fccb1d\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\n\n","type":"content","url":"/nlp-p02#tf-idf-vectorization","position":23},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization","lvl4":"Train the Vectorizer and Transform the Data","lvl3":"TF-IDF Vectorization","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"type":"lvl4","url":"/nlp-p02#train-the-vectorizer-and-transform-the-data","position":24},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization","lvl4":"Train the Vectorizer and Transform the Data","lvl3":"TF-IDF Vectorization","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"content":"\n\n# you may use this as input to fit the TF-IDF vectorizer\nraw_docs = sou[\"Text\"].to_list()\n\n# Hint - use fit_transform for vectorizer and PCA\n\n# Initialize TF-IDF vectorizer (default tokenizer is fine)\ntfidf_model = TfidfVectorizer()\n\n# Fit the model and transform the speeches into TF-IDF vectors\ntfidf_vectors = tfidf_model.fit_transform(raw_docs).toarray()\n\nprint(f\"TF-IDF matrix shape: {tfidf_vectors.shape}\")\n\n\nThe output of fit_transform() will be a matrix where each row corresponds to a speech, each column corresponds to a word in the corpus of speeches, and the value is the TF-IDF score which measures the importance of that word in that speech, relative to the rest of the speeches.\n\n","type":"content","url":"/nlp-p02#train-the-vectorizer-and-transform-the-data","position":25},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization","lvl4":"Plot Speeches","lvl3":"TF-IDF Vectorization","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"type":"lvl4","url":"/nlp-p02#plot-speeches","position":26},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization","lvl4":"Plot Speeches","lvl3":"TF-IDF Vectorization","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"content":"First used PCA to generate the first chart\n\nSecond use seaborn heatmap with a log-scaled color axis to generate the second chart\n\n# Step 1: Set PCA to find first 2 principal components\npca = PCA(n_components=2)\n\n# Step 2: Create a new dataframe where each row is a speech, and each column is a projection onto\n# one of the two principal components\ndf2d = pd.DataFrame(pca.fit_transform(tfidf_vectors), columns=[\"x\", \"y\"])\n\nplt.figure(figsize=(8, 6))\nplt.scatter(df2d[\"x\"], df2d[\"y\"], alpha=0.7)\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.title(\"PCA Plot of Vectorized Speeches (TF-IDF)\")\nplt.tight_layout()\nplt.savefig(\"outputs/vectorized_speeches_principal_components.png\")\nplt.show()\n\n# Hint - vectorized_docs is a sparse matrix whose rows are speeches and columns are tokens, with each\n# value being a TF-IDF score. Densify this array first, and then plot using seaborn.\n# TF-IDF HEATMAP WITH LOG-SCALED COLOR AXIS\nfrom matplotlib.colors import LogNorm\n\n# Convert sparse matrix â†’ dense matrix\ndense_tfidf = tfidf_vectors  # Already dense if you used .toarray() earlier\n# If yours is sparse, uncomment:\n# dense_tfidf = tfidf_vectors.toarray()\n\nplt.figure(figsize=(7, 5))\n\nsns.heatmap(\n    dense_tfidf,\n    norm=LogNorm(),\n    cmap=\"magma\",\n)\n\nplt.title(\"Vectorized Speeches\")\nplt.xlabel(\"Vector Index\")\nplt.ylabel(\"Speech Index\")\n\nplt.tight_layout()\nplt.savefig(\"outputs/2.3_Vectorized_Speeches.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n","type":"content","url":"/nlp-p02#plot-speeches","position":27},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization","lvl4":"Get the TF-IDF value for certain words and documents","lvl3":"TF-IDF Vectorization","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"type":"lvl4","url":"/nlp-p02#get-the-tf-idf-value-for-certain-words-and-documents","position":28},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization","lvl4":"Get the TF-IDF value for certain words and documents","lvl3":"TF-IDF Vectorization","lvl2":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"content":"\n\nword_list = ['year',\n 'america',\n 'people',\n 'american',\n 'work',\n 'new',\n 'job',\n 'country',\n 'americans',\n 'world'] # top ten most common words through whole corpus\n\nword_nums = tfidf_model.vocabulary_ # get each word's index number using the .vocabular_ attributed of vectorizer\n\nidf_score = []\nfor word in word_list:\n    idx = word_nums[word]\n    idf_score.append(tfidf_model.idf_[idx]) # get their IDF score by using .idf_ at the indices from the previous step\n\ntf_idf = []\nfor word in word_list:\n    idx = word_nums[word]\n    tf_idf.append(tfidf_vectors[0, idx])\n\n\npd.DataFrame({\"Word\": word_list, \"IDF Score\": idf_score, \"TF-IDF Score\": tf_idf})","type":"content","url":"/nlp-p02#get-the-tf-idf-value-for-certain-words-and-documents","position":29},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)"},"type":"lvl1","url":"/nlp-p03","position":0},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)"},"content":"","type":"content","url":"/nlp-p03","position":1},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl3":"Read Data"},"type":"lvl3","url":"/nlp-p03#read-data","position":2},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl3":"Read Data"},"content":"\n\n# imports\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\n\nplt.style.use('seaborn-v0_8-dark') \n\n# Set plotting style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.dpi'] = 100\n\n# Create outputs directory\nPath(\"outputs\").mkdir(exist_ok=True)\n\n# read in SOTU.csv using pandas, name the variable `sou` for simplicity\n# the below cell is what the output should look like\n# read in SOTU.csv \nsou = pd.read_csv('data/SOTU.csv')\nsou.head()\n\nimport spacy\nfrom tqdm import tqdm\nfrom collections import Counter\n\nnlp = spacy.load(\"en_core_web_sm\")\n\n","type":"content","url":"/nlp-p03#read-data","position":3},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl2":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)"},"type":"lvl2","url":"/nlp-p03#part-3-advanced-text-processing-lda-and-bertopic-topic-modeling-20-pts","position":4},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl2":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)"},"content":"Resources:\n\nLDA:\n\nhttps://â€‹mediumâ€‹.comâ€‹/sayahfares19â€‹/textâ€‹-analysisâ€‹-topicâ€‹-modellingâ€‹-withâ€‹-spacyâ€‹-gensimâ€‹-4cd92ef06e06\n\nhttps://â€‹wwwâ€‹.kaggleâ€‹.comâ€‹/codeâ€‹/faressayahâ€‹/textâ€‹-analysisâ€‹-topicâ€‹-modelingâ€‹-withâ€‹-spacyâ€‹-gensimâ€‹#ğŸ“šâ€‹-Topicâ€‹-Modeling (code for previous post)\n\nhttps://â€‹towardsdatascienceâ€‹.comâ€‹/topicâ€‹-modellingâ€‹-inâ€‹-pythonâ€‹-withâ€‹-spacyâ€‹-andâ€‹-gensimâ€‹-dc8f7748bdbf/\n\nBERTopic:\n\nhttps://â€‹maartengrâ€‹.githubâ€‹.ioâ€‹/BERTopicâ€‹/gettingâ€‹_startedâ€‹/visualizationâ€‹/visualizeâ€‹_documentsâ€‹.htmlâ€‹#visualizeâ€‹-documentsâ€‹-withâ€‹-plotly\n\nhttps://â€‹maartengrâ€‹.githubâ€‹.ioâ€‹/BERTopicâ€‹/gettingâ€‹_startedâ€‹/visualizationâ€‹/visualizeâ€‹_topicsâ€‹.html\n\nfrom spacy import displacy\nfrom bertopic import BERTopic\nfrom gensim.corpora import Dictionary\nfrom gensim.models import LdaModel\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pyLDAvis\nimport pyLDAvis.gensim_models\nimport numpy as np\n\n","type":"content","url":"/nlp-p03#part-3-advanced-text-processing-lda-and-bertopic-topic-modeling-20-pts","position":5},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl3":"LDA","lvl2":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)"},"type":"lvl3","url":"/nlp-p03#lda","position":6},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl3":"LDA","lvl2":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)"},"content":"Train an LDA model with 18 topics\n\nOutput the top 10 words for each topic.\n\nOutput the topic distribution for the first speech\n\nMake a visualization\n\nYou may use the next two cells to process the data.\n\ndef preprocess_text(text): \n    doc = nlp(text) \n    return [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and not token.is_space and len(token.lemma_) > 3]\n\n# Process all texts - note this takes ~ 5 minutes to run\nprocessed_docs = sou['Text'].apply(preprocess_text)\n\nTo train an LDA model, use the LdaModel function that we imported a couple of cells back. The last resource linked under the LDA section is especially useful for walking through the steps we have below. Note: one of the arguments to the LdaModel function is random_state which specifies the random seed for reproducibility. Please set yours to 42. Further, the last resource provided uses LdaMulticore which is essentially a parallelizable version of our function LdaModel. Use LdaModel instead, but the usage will be similar, except you can ignore the iterations and workers arguments...\n\n# Build dictionary from processed_docs, which is a list of tokens extracted from our speeches\ndictionary = Dictionary(processed_docs)\ncorpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n\n# train LDA model with 18 topics\nlda_model = LdaModel(\n    corpus=corpus,\n    id2word=dictionary,\n    num_topics=18,\n    random_state=42\n)\n\n# print the top 10 words for each topic\nfor idx in range(18):\n    topic_words = lda_model.show_topic(idx, 10)\n    print(f\"Topic: {idx}\")\n    words = \" + \".join([f\"{float(prob):.3f}*\\\"{word}\\\"\" for word, prob in topic_words])\n    print(f\"Words: {words}\\n\")\n\n# print the topic distribution for the first speech\nfirst_speech_topics = lda_model.get_document_topics(corpus[0])\nprint(first_speech_topics)\n\n# make a visualization using pyLDAvis\npyLDAvis.enable_notebook()\npyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n\n\n","type":"content","url":"/nlp-p03#lda","position":7},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl3":"BERTopic","lvl2":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)"},"type":"lvl3","url":"/nlp-p03#bertopic","position":8},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl3":"BERTopic","lvl2":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)"},"content":"Train a BERTopic model with a min_topic_size of 3 Hint: use BERTopic to instantiate the model and specify min_topic_size in here. Actually fit the model using fit_transform, which docs passed into this.\n\nOutput the top 10 words for each topic.\n\nOutput the topic distribution for the first speech\n\nMake a visualization of the topics (see topic_model.visualize_topics())\n\ndocs = sou['Text'].to_list()\n\n# train the model - this takes about 30 seconds\ntopic_model = BERTopic(min_topic_size=3)\ntopics, probabilities = topic_model.fit_transform(docs)\n\n# remove stop words from the topics (Hint: use CountVectorizer and then .update_topics on topic_model)\nvectorizer_model = CountVectorizer(stop_words=\"english\")\ntopic_model.update_topics(docs, vectorizer_model=vectorizer_model)\n\n# output the top 10 words for each topic - hint see get_topic_info\ntopic_model.get_topic_info()\n\n# output the topic distribution for the first speech\ntopic_distribution = topic_model.approximate_distribution(docs[0])[0]\n\nprobabilities = topic_distribution[0]\n\ntopic_info = topic_model.get_topic_info()\ntopic_labels = []\nfor i in range(len(probabilities)):\n    topic_label = topic_info[topic_info['Topic'] == i]['Name'].values\n    if len(topic_label) > 0:\n        topic_labels.append(topic_label[0])\n    else:\n        topic_labels.append(f\"Topic {i}\")\n\nplt.figure(figsize=(12, 8))\ny_pos = np.arange(len(probabilities))\nplt.barh(y_pos, probabilities)\nplt.yticks(y_pos, topic_labels)\nplt.xlabel('Probability')\nplt.title('Topic Probability Distribution')\nplt.tight_layout()\nplt.show()\n\n# run this cell to visualize the topics\ntopic_model.visualize_topics()","type":"content","url":"/nlp-p03#bertopic","position":9},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing"},"type":"lvl1","url":"/nlp-p04","position":0},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing"},"content":"","type":"content","url":"/nlp-p04","position":1},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"Read Data"},"type":"lvl3","url":"/nlp-p04#read-data","position":2},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl3":"Read Data"},"content":"\n\n# imports\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('seaborn-v0_8-dark') \n\n# read in SOTU.csv \nsou = pd.read_csv('data/SOTU.csv')\nsou.head()\n\n\n\n","type":"content","url":"/nlp-p04#read-data","position":3},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl2":"Part 4: Choose your own advecnture! (7 Points; Optional for Extra Credit)"},"type":"lvl2","url":"/nlp-p04#part-4-choose-your-own-advecnture-7-points-optional-for-extra-credit","position":4},{"hierarchy":{"lvl1":"Project 2: Reproducibility in Natural Language Processing","lvl2":"Part 4: Choose your own advecnture! (7 Points; Optional for Extra Credit)"},"content":"This section is open ended and your chance to explare any advanced analysis. Please perform any additional analysis you find interesting! Suggested analyses (only do one max):\n\nTopic evolution over time - see \n\nhttps://â€‹maartengrâ€‹.githubâ€‹.ioâ€‹/BERTopicâ€‹/gettingâ€‹_startedâ€‹/topicsovertimeâ€‹/topicsovertimeâ€‹.htmlâ€‹#visualization\n\nWord frequency over time - does the frequency of certain words change over time\n\nSemantic similarity - investigate similarity within and between presidents or time periods. For example, similarity between one presidents speeches, e.g. are all of Bidenâ€™s speeches similar to each other? How similar are they to Trumpâ€™s speeches? Are speeches from the 2000s more similar to each other than speeches in the 1800s? Which two presidents have the most similar speeches? See \n\nhttps://â€‹spacyâ€‹.ioâ€‹/usageâ€‹/linguisticâ€‹-featuresâ€‹#vectorsâ€‹-similarity\n\nNamed Entity Recognition - which entity types are most common in speeches? What are the most common words for each entity type - see \n\nhttps://â€‹spacyâ€‹.ioâ€‹/usageâ€‹/linguisticâ€‹-featuresâ€‹#namedâ€‹-entities\n\nClassification - can you build a classifier to detect democratic versus republican state of the union speeches from 1980-2024 - see \n\nhttps://â€‹scikitâ€‹-learnâ€‹.orgâ€‹/stableâ€‹/autoâ€‹_examplesâ€‹/textâ€‹/plotâ€‹_documentâ€‹_classificationâ€‹_20newsgroupsâ€‹.htmlâ€‹#sphxâ€‹-glrâ€‹-autoâ€‹-examplesâ€‹-textâ€‹-plotâ€‹-documentâ€‹-classificationâ€‹-20newsgroupsâ€‹-py\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport pandas as pd\n\n\ndef plot_word_frequency_over_time(df, target_words):\n    data_rows = []\n    for index, row in df.iterrows():\n        year = row['Year']\n        text = row['Text'].lower() \n        \n        \n        total_words = len(text.split())\n        \n        if total_words == 0: continue\n            \n        for word in target_words:\n           \n            count = text.count(word)\n            \n            freq_per_1k = (count / total_words) * 1000\n            \n            data_rows.append({\n                'Year': year,\n                'Word': word,\n                'Frequency': freq_per_1k\n            })\n            \n    freq_df = pd.DataFrame(data_rows)\n    \n    plt.figure(figsize=(14, 7))\n    sns.lineplot(data=freq_df, x='Year', y='Frequency', hue='Word', linewidth=2)\n    \n    plt.title('Word Frequency Over Time (per 1,000 words)', fontsize=14)\n    plt.ylabel('Frequency per 1,000 words', fontsize=12)\n    plt.xlabel('Year', fontsize=12)\n    plt.grid(True, alpha=0.3)\n    plt.legend(title='Target Word')  \n    plt.subplots_adjust(hspace=0.5)\n    plt.tight_layout()\n    save_path = \"outputs/Part_4_plot_word_frequency_over_time.png\"\n    plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n    print(save_path)\n    plt.show()\n\n\n\ninteresting_words = ['immigrant', 'economy', 'climate', 'china', 'japan']\n\nplot_word_frequency_over_time(sou, interesting_words)\nplt.savefig(\"save_filename\", dpi=300, bbox_inches=\"tight\")\n\nThis section details the â€œWord Frequency Over Timeâ€ analysis, which tracks the normalized usage of five key terms: â€œimmigrant,â€ â€œeconomy,â€ â€œclimate,â€ â€œchina,â€ and â€œjapanâ€, across addresses to ensure fair comparison despite varying speech lengths. The resulting timeseries plot reveals distinct historical patterns: â€œJapanâ€ exhibits a massive spike in the early 1940s corresponding to World War II, while â€œChinaâ€ shows a steady rise beginning in the late 20th century, reflecting its growing geopolitical significance. â€œEconomyâ€ displays a cyclical pattern with peaks aligning with recessions, suggesting it is a reactive topic emphasized during downturns. Modern issues like â€œclimateâ€ appear only in recent decades, marking a shift in political priorities, while â€œimmigrantâ€ usage spikes in the modern era, likely tracking legislative debates. The sharp,  for foreign policy terms contrast with the recurrent, moderate peaks for domestic economic concerns, demonstrating the State of the Unionâ€™s role as a historical of shifting national priorities. The generated visualization is saved as outputs/4_1_Word_Frequency_Over_Time.png, with the underlying code documented in the nlp-P04.ipynb notebook.","type":"content","url":"/nlp-p04#part-4-choose-your-own-advecnture-7-points-optional-for-extra-credit","position":5}]}