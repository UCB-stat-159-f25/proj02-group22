{"version":2,"kind":"Notebook","sha256":"5117091ec57ed018aed71a18b646afc80a080da8c5e08d893a1e83677c9448a1","slug":"nlp-p02","location":"/nlp-P02.ipynb","dependencies":[],"frontmatter":{"title":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"github":"https://github.com/UCB-stat-159-f25/proj02-group22","source_url":"https://github.com/UCB-stat-159-f25/proj02-group22/blob/main/nlp-P02.ipynb","edit_url":"https://github.com/UCB-stat-159-f25/proj02-group22/edit/main/nlp-P02.ipynb","exports":[{"format":"ipynb","filename":"nlp-P02.ipynb","url":"/user/yau_christy/myst-build/proj02-group22/build/nlp-P02-d9a3fd3c40f301dd32252c8377f43212.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Read Data","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"siSvPqYaEK"}],"identifier":"read-data","label":"Read Data","html_id":"read-data","implicit":true,"key":"fME0J7IEeG"}],"key":"Xu9g4wHjlj"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# imports\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('seaborn-v0_8-dark') ","key":"CeWvar57O9"},{"type":"output","id":"4X2GK_aKkjGDFveIz3aUR","data":[],"key":"JGsap6p7ME"}],"key":"rXEUPD6fyH"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# read in SOTU.csv using pandas, name the variable `sou` for simplicity\n# the below cell is what the output should look like\n# read in SOTU.csv \nsou = pd.read_csv('data/SOTU.csv')\nsou.head()","key":"Z0JFSP2CYp"},{"type":"output","id":"SJtPr6WOQfvcBdzqkJVSJ","data":[{"output_type":"execute_result","execution_count":32,"metadata":{},"data":{"text/html":{"content":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>President</th>\n      <th>Year</th>\n      <th>Text</th>\n      <th>Word Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Joseph R. Biden</td>\n      <td>2024.0</td>\n      <td>\\n[Before speaking, the President presented hi...</td>\n      <td>8003</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Joseph R. Biden</td>\n      <td>2023.0</td>\n      <td>\\nThe President. Mr. Speaker——\\n[At this point...</td>\n      <td>8978</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Joseph R. Biden</td>\n      <td>2022.0</td>\n      <td>\\nThe President. Thank you all very, very much...</td>\n      <td>7539</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Joseph R. Biden</td>\n      <td>2021.0</td>\n      <td>\\nThe President. Thank you. Thank you. Thank y...</td>\n      <td>7734</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Donald J. Trump</td>\n      <td>2020.0</td>\n      <td>\\nThe President. Thank you very much. Thank yo...</td>\n      <td>6169</td>\n    </tr>\n  </tbody>\n</table>\n</div>","content_type":"text/html"},"text/plain":{"content":"         President    Year                                               Text  \\\n0  Joseph R. Biden  2024.0  \\n[Before speaking, the President presented hi...   \n1  Joseph R. Biden  2023.0  \\nThe President. Mr. Speaker——\\n[At this point...   \n2  Joseph R. Biden  2022.0  \\nThe President. Thank you all very, very much...   \n3  Joseph R. Biden  2021.0  \\nThe President. Thank you. Thank you. Thank y...   \n4  Donald J. Trump  2020.0  \\nThe President. Thank you very much. Thank yo...   \n\n   Word Count  \n0        8003  \n1        8978  \n2        7539  \n3        7734  \n4        6169  ","content_type":"text/plain"}}}],"key":"oEeG6cHaTS"}],"key":"XEZPBTIqAP"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iDR7a3MAQq"}],"identifier":"part-2-simple-text-processing-tokenization-lemmatization-word-frequency-vectorization-20-pts","label":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","html_id":"part-2-simple-text-processing-tokenization-lemmatization-word-frequency-vectorization-20-pts","implicit":true,"key":"dHFAdIDVvp"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Now we will start working on simple text processing using the ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"C4YiUMMmEe"},{"type":"inlineCode","value":"SpaCy","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"nuOX3sJ5UZ"},{"type":"text","value":" package and the same dataset as Part 1. The package should already be included in the ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"hyNhVluYPL"},{"type":"inlineCode","value":"environment.yml","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"rXSGpcUBwo"},{"type":"text","value":". However, we will also need to download ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"xc7FF7FDOP"},{"type":"inlineCode","value":"en_core_web_sm","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"YwZqWE2EOj"},{"type":"text","value":", an English language text processing model. To do this, while having your ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"DvKm9gEXQo"},{"type":"inlineCode","value":"sotu","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"KoBSA6n3c2"},{"type":"text","value":" environment activated, run the following:","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"SKd6U2535K"}],"key":"ecDGQI0pr4"},{"type":"code","lang":"","value":"python -m spacy download en_core_web_sm","position":{"start":{"line":5,"column":1},"end":{"line":7,"column":1}},"key":"RSmMA9bS3y"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Now, you should be good to go!","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"PRPTwhBmL4"}],"key":"dyuphqo5Ps"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Some important definitions:","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"FRUJa0LimG"}],"key":"JhG9BmbadP"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":13,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"emphasis","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Token","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"M8ANAvrxrO"}],"key":"JT2C7xSC7m"},{"type":"text","value":": a single word or piece of a word","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"SWM1nVV6IA"}],"key":"KilQSnsYws"}],"key":"g1pMkYHGsw"},{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"emphasis","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"Lemma","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"llEkRV5qhT"}],"key":"hxHD5Ln1kE"},{"type":"text","value":": the core component of a word, e.g., “complete” is the lemma for “completed” and “completely”","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"B5gohOrdJF"}],"key":"nVm3FA9Ssq"}],"key":"aLNuOLBcyQ"},{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","children":[{"type":"emphasis","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"Stop Word","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"XaPbjPTTPb"}],"key":"MkLioHqWLC"},{"type":"text","value":": a common word that does not add semantic value, such as “a”, “and”, “the”, etc.","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"QmyeZ7MX1w"}],"key":"WWJMYZz2BD"}],"key":"jzPQGRhOyZ"},{"type":"listItem","spread":true,"position":{"start":{"line":16,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","children":[{"type":"emphasis","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"Vectorization","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"AWOmjKY5q3"}],"key":"hL6dmMh9Om"},{"type":"text","value":": representing a document as a vector where each index in the vector corresponds to a token or word and each entry is the count.","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"iWTRzp7t57"}],"key":"XWx0R7j8bN"}],"key":"my33hB0A9C"}],"key":"Myl57qL9lM"},{"type":"paragraph","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"text","value":"In this section, we will explore the most common tokens and lemmas throughout different slices of the speech data. We will also develop vectorization representations of the speeches.","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"gpP1dpBsrn"}],"key":"ntaZFRfVM1"},{"type":"paragraph","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"text","value":"The core steps are:","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"L1jBGf0QaT"}],"key":"wyWxIKyW8q"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":22,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Process speeches using the SpaCy nlp module","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"c1cxUGZLq5"}],"key":"vTGfhWleSH"}],"key":"zb7l4feAQB"},{"type":"listItem","spread":true,"position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Analyze Tokens vs Lemmas:","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"gT1PJUU7yd"}],"key":"VXQMXjdg18"}],"key":"l0eP7x00c0"}],"key":"NJVe9c8H4y"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":24,"column":1},"end":{"line":26,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Create a list of all tokens across all speeches that are not stop words, punctuation, or spaces.","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"kU6yEiuax1"}],"key":"PJlCzkrpee"}],"key":"dyDw44au0w"},{"type":"listItem","spread":true,"position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Create a second list of the lemmas for these same tokens.","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"SoK8UI6ZS7"}],"key":"HAcJUSicfN"}],"key":"rZoO14TlQL"},{"type":"listItem","spread":true,"position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Display the top 25 for each of these and compare.","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"key":"Zx5e3NIJV3"}],"key":"SNQptwQHQN"}],"key":"zLAn7Gn6Gb"}],"key":"pWlhx4zn0q"},{"type":"list","ordered":true,"start":3,"spread":false,"position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Analyze common word distributions over different years:","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"WwLPO9eVaQ"}],"key":"XFrT3jbAZR"}],"key":"uKQXWmhean"}],"key":"eYao9PWRzU"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":28,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Create a function that takes the dataset and a year as an input and outputs the top n lemmas for that year’s speeches","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"key":"FT3XtdPFaD"}],"key":"NyelTFIt9e"}],"key":"lMFyhhSNlw"},{"type":"listItem","spread":true,"position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Compare the top 10 words for 2023 versus 2019","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"WotvjBUkiU"}],"key":"ZJ93TEK8H0"}],"key":"JaiCvOp0xL"}],"key":"kj2auXSKu6"},{"type":"list","ordered":true,"start":4,"spread":false,"position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Document Vectorization:","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"key":"cn1slkdaxX"}],"key":"ZxwIbJUEZN"}],"key":"YX9EUbNbrg"}],"key":"JsR8fifgBs"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":31,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Train a Term Frequency-Inverse Document Frequency (TF-IDF) vectorization model using your processed dataset and scikit learn","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"aXOYLXlQyW"}],"key":"KgXSJCqSUi"}],"key":"p0Y2eQmEFK"},{"type":"listItem","spread":true,"position":{"start":{"line":32,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Output the feature vectors","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"key":"y2NhDBuj10"}],"key":"E5aQViua6Z"}],"key":"wyRZ6hcNgi"}],"key":"eTnChiAMBV"},{"type":"paragraph","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"children":[{"type":"strong","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"children":[{"type":"text","value":"Helpful Resources:","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"key":"OWrYSgTDpP"}],"key":"pS4PCXfHhA"}],"key":"LKHgIUJoU0"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":35,"column":1},"end":{"line":39,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"paragraph","children":[{"type":"link","url":"https://realpython.com/natural-language-processing-spacy-python/","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"text","value":"https://​realpython​.com​/natural​-language​-processing​-spacy​-python/","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"Mr3CL8D7GE"}],"urlSource":"https://realpython.com/natural-language-processing-spacy-python/","key":"yzHWsMeXxv"}],"key":"SM1OnQ2UzM"}],"key":"nWOP9MIW7N"},{"type":"listItem","spread":true,"position":{"start":{"line":36,"column":1},"end":{"line":36,"column":1}},"children":[{"type":"paragraph","children":[{"type":"link","url":"https://www.statology.org/text-preprocessing-feature-engineering-spacy/","position":{"start":{"line":36,"column":1},"end":{"line":36,"column":1}},"children":[{"type":"text","value":"https://​www​.statology​.org​/text​-preprocessing​-feature​-engineering​-spacy/","position":{"start":{"line":36,"column":1},"end":{"line":36,"column":1}},"key":"O9UROFQJCy"}],"urlSource":"https://www.statology.org/text-preprocessing-feature-engineering-spacy/","key":"KrxMWiM2QH"}],"key":"mTdf9MTAdX"}],"key":"m6KkkHtE0e"},{"type":"listItem","spread":true,"position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"paragraph","children":[{"type":"link","url":"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"https://​scikit​-learn​.org​/stable​/modules​/generated​/sklearn​.feature​_extraction​.text​.TfidfVectorizer​.html#","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"Um27ozjEBT"}],"urlSource":"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#","key":"gk17R5LofV"}],"key":"RzZZZ4sE4U"}],"key":"lkMB8c2EQj"},{"type":"listItem","spread":true,"position":{"start":{"line":38,"column":1},"end":{"line":39,"column":1}},"children":[{"type":"paragraph","children":[{"type":"link","url":"https://www.geeksforgeeks.org/nlp/how-to-store-a-tfidfvectorizer-for-future-use-in-scikit-learn/","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"children":[{"type":"text","value":"https://​www​.geeksforgeeks​.org​/nlp​/how​-to​-store​-a​-tfidfvectorizer​-for​-future​-use​-in​-scikit​-learn/","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"key":"XOdVqQJ6Rx"}],"urlSource":"https://www.geeksforgeeks.org/nlp/how-to-store-a-tfidfvectorizer-for-future-use-in-scikit-learn/","key":"ALcNN41PBC"}],"key":"IvZeei9POu"}],"key":"GMuXoTxfU1"}],"key":"EpaeUeQXz0"}],"key":"DUgxcf1iMt"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Processing Speeches with SpaCy","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"plnQRyKC4W"}],"identifier":"processing-speeches-with-spacy","label":"Processing Speeches with SpaCy","html_id":"processing-speeches-with-spacy","implicit":true,"key":"UFwdcx7JPy"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Lets study just speeches from 2000 and onwards to begin with. So, be sure to subset your DataFrame to just these speeches before continuing!","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"h1AlbiieA1"}],"key":"qngnWVayTj"}],"key":"ZPcFFyht9Q"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"!pip install spacy\n!python -m spacy download en_core_web_sm","key":"uECnzHGMHp"},{"type":"output","id":"Cwjr502c1vqzgk-kn7lX0","data":[{"name":"stdout","output_type":"stream","text":"Requirement already satisfied: spacy in /srv/conda/envs/notebook/lib/python3.12/site-packages (3.8.11)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (1.0.15)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.0.13)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (3.0.12)\nRequirement already satisfied: thinc<8.4.0,>=8.3.4 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (8.3.10)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.5.2)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.4.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (0.4.3)\nRequirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (0.20.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (4.67.1)\nRequirement already satisfied: numpy>=1.19.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (1.26.4)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (2.12.3)\nRequirement already satisfied: jinja2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (3.1.6)\nRequirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (70.0.0)\nRequirement already satisfied: packaging>=20.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from spacy) (24.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.4 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\nRequirement already satisfied: typing-extensions>=4.14.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\nRequirement already satisfied: typing-inspection>=0.4.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.20)\nRequirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\nRequirement already satisfied: blis<1.4.0,>=1.3.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\nRequirement already satisfied: click>=8.0.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.0)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\nRequirement already satisfied: wrapt in /srv/conda/envs/notebook/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (1.17.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\nCollecting en-core-web-sm==3.8.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n"}],"key":"Oq1bPFzp1k"}],"key":"X05y4njyEz"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import spacy\nfrom tqdm import tqdm\nfrom collections import Counter\n\nnlp = spacy.load(\"en_core_web_sm\")","key":"DXL7mlS5CW"},{"type":"output","id":"FMePvRDA8naqAcI0aJOLf","data":[],"key":"E5TDrVZpEn"}],"key":"OwQZoWRLuu"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Load the SOTU dataset\nsotu = pd.read_csv('data/SOTU.csv')\n\nprint(f\"\\nDataset loaded successfully!\")\nprint(f\"Shape: {sotu.shape}\")\nprint(f\"Columns: {sotu.columns.tolist()}\")\nprint(f\"Total speeches: {len(sotu)}\")\nprint(f\"Year range: {sotu['Year'].min()} - {sotu['Year'].max()}\")","key":"zGnQRzQNHb"},{"type":"output","id":"nvEK9QurBoBmbpu6bLjNL","data":[{"name":"stdout","output_type":"stream","text":"\nDataset loaded successfully!\nShape: (246, 4)\nColumns: ['President', 'Year', 'Text', 'Word Count']\nTotal speeches: 246\nYear range: 1790.0 - 2024.0\n"}],"key":"BjpU6jaMC9"}],"key":"PALRJFt6fb"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# subset the speech dataframe for speeches from 2000 and onwards\nprint(\"SUBSETTING DATA (2000 ONWARDS)\")\n\nsotu_2000 = sotu[sotu['Year'] >= 2000].copy()\n\nprint(f\"\\nFiltered dataset for analysis:\")\nprint(f\"  Speeches from 2000+: {len(sotu_2000)}\")\nprint(f\"  Year range: {sotu_2000['Year'].min()} - {sotu_2000['Year'].max()}\")\n","key":"FePw387P38"},{"type":"output","id":"-nsjcOaKxzz2dzvFI8Ayv","data":[{"name":"stdout","output_type":"stream","text":"SUBSETTING DATA (2000 ONWARDS)\n\nFiltered dataset for analysis:\n  Speeches from 2000+: 25\n  Year range: 2000.0 - 2024.0\n"}],"key":"Nj5N9LklfA"}],"key":"lx4tLUMHrr"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"print(\"PROCESSING SPEECHES WITH SPACY (2000+ only)\")\n\n# Process speeches from 2000+ using nlp.pipe() for efficiency\nprocessed_docs = []\n\nfor doc in tqdm(nlp.pipe(sotu_2000['Text'].tolist(), batch_size=50), \n                total=len(sotu_2000), \n                desc=\"Processing speeches\"):\n    processed_docs.append(doc)","key":"NRX7sYhjwP"},{"type":"output","id":"9qHxZ0nRgzAAR50bBtKZV","data":[{"name":"stdout","output_type":"stream","text":"PROCESSING SPEECHES WITH SPACY (2000+ only)\n"},{"name":"stderr","output_type":"stream","text":"Processing speeches: 100%|██████████| 25/25 [00:23<00:00,  1.07it/s]\n"}],"key":"K6MDGoj2Fy"}],"key":"pfwhLMecMc"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Analyze Tokens vs Lemmas","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"b1ahYyCwzd"}],"identifier":"analyze-tokens-vs-lemmas","label":"Analyze Tokens vs Lemmas","html_id":"analyze-tokens-vs-lemmas","implicit":true,"key":"xNv6syPAxt"},{"type":"heading","depth":4,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Token List","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"mCWAmS0rS3"}],"identifier":"token-list","label":"Token List","html_id":"token-list","implicit":true,"key":"QodalkWuBv"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Create a list of tokens across all speeches that are not spaces, stopwords, or punctuation. Make each token lowercase as well. ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"GrgjfZtyWr"},{"type":"emphasis","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Hint: each element of the list we just created are themselves lists of tokens. Token objects have attributes ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"ShrHCY0UzZ"},{"type":"inlineCode","value":"is_stop","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"fWVVmz1Qxu"},{"type":"text","value":", ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"DmQnO7nTC3"},{"type":"inlineCode","value":"is_punct","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"jsvMV4Mr4C"},{"type":"text","value":", and ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"kyZSB5eYMK"},{"type":"inlineCode","value":"is_space","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"LHwsSI10Rr"},{"type":"text","value":".","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"pxuVHPU0g8"}],"key":"ipIFHhy54n"}],"key":"AWQDt2OBG5"}],"key":"vTzlEp3dky"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"print(\"ANALYZE TOKENS VS LEMMAS\")\n\n# TOKEN LIST\n\nprint(\"\\n--- Creating Token List ---\")\n\nall_tokens = []\n\nfor doc in processed_docs:\n    for token in doc:\n        if not token.is_stop and not token.is_punct and not token.is_space:\n            all_tokens.append(token.text.lower())\n\ntoken_counts = Counter(all_tokens)\ntop_25_tokens = token_counts.most_common(20)\ntop_25_tokens","key":"mhxXIXYC6q"},{"type":"output","id":"bgr4aVaN52pUZX5cP43W5","data":[{"name":"stdout","output_type":"stream","text":"ANALYZE TOKENS VS LEMMAS\n\n--- Creating Token List ---\n"},{"output_type":"execute_result","execution_count":38,"metadata":{},"data":{"text/plain":{"content":"[('america', 816),\n ('people', 637),\n ('american', 582),\n ('new', 530),\n ('years', 439),\n ('americans', 437),\n ('world', 425),\n ('year', 406),\n ('country', 369),\n ('jobs', 348),\n ('tonight', 344),\n ('work', 324),\n ('know', 323),\n ('let', 320),\n ('congress', 317),\n ('nation', 311),\n ('time', 301),\n ('help', 282),\n ('need', 266),\n ('tax', 255)]","content_type":"text/plain"}}}],"key":"pdqHAz4PDH"}],"key":"pnNH4V76mz"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":4,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Lemma List","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MplMD7QHde"}],"identifier":"lemma-list","label":"Lemma List","html_id":"lemma-list","implicit":true,"key":"PuUtmYXWgV"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Do the same as above, but for lemmas. ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"UvtVoNp1MD"},{"type":"emphasis","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Hint: recall lemmas are components of words. Each token should have an attribute to extract the lemma.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"bDRyT6jImr"}],"key":"U0FZeBdPUv"}],"key":"FOlA9ni5AN"}],"key":"tPagEAXrQO"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"all_lemmas = []\n\nfor doc in processed_docs:\n    for token in doc:\n        if not token.is_stop and not token.is_punct and not token.is_space:\n            all_lemmas.append(token.lemma_.lower())\n\nlemma_counts = Counter(all_lemmas)\ntop_25_lemmas = lemma_counts.most_common(25)\ntop_25_lemmas","key":"kErUfeF418"},{"type":"output","id":"oItdCoj4KYW-2wtVl0LCd","data":[{"output_type":"execute_result","execution_count":39,"metadata":{},"data":{"text/plain":{"content":"[('year', 845),\n ('america', 816),\n ('people', 639),\n ('american', 587),\n ('work', 557),\n ('new', 532),\n ('job', 486),\n ('country', 435),\n ('americans', 432),\n ('world', 426),\n ('know', 395),\n ('nation', 388),\n ('help', 378),\n ('need', 353),\n ('time', 351),\n ('tonight', 344),\n ('child', 332),\n ('let', 326),\n ('congress', 317),\n ('come', 301),\n ('family', 296),\n ('good', 294),\n ('right', 282),\n ('million', 274),\n ('want', 264)]","content_type":"text/plain"}}}],"key":"VA3rIxsyI3"}],"key":"yZ8xJeJylP"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":4,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Token versus Lemma Comparison","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sUM5wigla4"}],"identifier":"token-versus-lemma-comparison","label":"Token versus Lemma Comparison","html_id":"token-versus-lemma-comparison","implicit":true,"key":"yklB0Ouvf3"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"What do you notice about the top tokens versus the top lemmas?\nConsider two tokens - “year” and “years” - how do their counts compare to the lemma “year”?\nWhat about the lemma “child”?","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"kga5SoOWBd"}],"key":"KN644MVhhS"}],"key":"U5dfAtT1oc"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"print(\"\\n--- Token vs Lemma Comparison ---\")\n\n# Compare \"year\" and \"years\"\nyear_count = token_counts.get('year', 0)\nyears_count = token_counts.get('years', 0)\nyear_lemma_count = lemma_counts.get('year', 0)\n\nprint(f\"\\nComparison for 'year'/'years':\")\nprint(f\"  Token 'year': {year_count}\")\nprint(f\"  Token 'years': {years_count}\")\nprint(f\"  Lemma 'year': {year_lemma_count}\")\n\n# Compare \"child\" and \"children\"\nchild_count = token_counts.get('child', 0)\nchildren_count = token_counts.get('children', 0)\nchild_lemma_count = lemma_counts.get('child', 0)\n\nprint(f\"\\nComparison for 'child'/'children':\")\nprint(f\"  Token 'child': {child_count}\")\nprint(f\"  Token 'children': {children_count}\")\nprint(f\"  Lemma 'child': {child_lemma_count}\")\n","key":"xwngljm4Pu"},{"type":"output","id":"GJ4vq5l9ceJkLC0jlB3Xs","data":[{"name":"stdout","output_type":"stream","text":"\n--- Token vs Lemma Comparison ---\n\nComparison for 'year'/'years':\n  Token 'year': 406\n  Token 'years': 439\n  Lemma 'year': 845\n\nComparison for 'child'/'children':\n  Token 'child': 119\n  Token 'children': 215\n  Lemma 'child': 332\n"}],"key":"X1W0u5asuU"}],"key":"unqTHZenEE"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Common Words","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"S4a8RFXtCa"}],"identifier":"common-words","label":"Common Words","html_id":"common-words","implicit":true,"key":"DQq7homIX2"},{"type":"heading","depth":4,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Common Words per Year Function","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"SBgpelR2QR"}],"identifier":"common-words-per-year-function","label":"Common Words per Year Function","html_id":"common-words-per-year-function","implicit":true,"key":"rSg9Oenp6m"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Fill in the below function to obtain the n-most common words in speeches for a given year.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"yTBKUVUnFf"}],"key":"dyN97OvUqm"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"inputs:","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"dxYUhKecD5"}],"key":"DDWozTIlYb"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":8,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"df raw unprocessed sou dataframe","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"UL70sSclYL"}],"key":"iNCZBlH0Z6"}],"key":"rXJbTsEySm"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"year","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"dPO5SuRu8p"}],"key":"EvD8HYlk3c"}],"key":"Gbe96kfDWk"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"n\noutputs:","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"g5FUXqSNke"}],"key":"ABW64PIKMr"}],"key":"oKAlIyBPRB"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"top n words for that years","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"MQFdefbg4J"}],"key":"wmL0GMZilS"}],"key":"bhKYPEudTz"}],"key":"fFUI72mUhP"},{"type":"paragraph","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"steps:","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"bUeLh9ObuQ"}],"key":"vcjp6VZcXL"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":15,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"subset the dataframe for the year of interest - note the years might not be in int type","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"pb0758DSXk"}],"key":"L3XSBDb9al"}],"key":"bVjcbM9c3M"},{"type":"listItem","spread":true,"position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"process the subsetted dataframe with spacy","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"hETzYfLOqx"}],"key":"uep7DmGoGy"}],"key":"Jf9X8MLIz8"},{"type":"listItem","spread":true,"position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"get the lemmas across all those speeches","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"wB5p7RBOxa"}],"key":"p6INBs457a"}],"key":"ISUYQuD6gZ"},{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"count the top n lemmas","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"Z12NMJG3A5"}],"key":"tLM29zP74X"}],"key":"BAUE5SatF8"}],"key":"e8niDYPct3"}],"key":"XCiXKRyqRF"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"print(\"COMMON WORDS ANALYSIS\")\n\ndef get_most_common_words(df, year, n=25):\n    \"\"\"\n    Get the n most common lemmas for a given year.\n    \n    Parameters:\n    -----------\n    df : DataFrame\n        SOTU dataframe with 'Year' and 'Text' columns\n    year : int or float\n        Year of interest\n    n : int\n        Number of top words to return\n        \n    Returns:\n    --------\n    list of tuples: (lemma, count) for top n words\n    \"\"\"\n    # Subset for the year\n    df_year = df[df['Year'] == year]\n    \n    if len(df_year) == 0:\n        print(f\"No speeches found for year {year}\")\n        return []\n    \n    # Process and extract lemmas\n    year_lemmas = []\n    for doc in nlp.pipe(df_year['Text'].tolist()):\n        for token in doc:\n            if not token.is_stop and not token.is_punct and not token.is_space:\n                year_lemmas.append(token.lemma_.lower())\n    \n    # Count and return top n\n    return Counter(year_lemmas).most_common(n)","key":"wFdZnBoNLR"},{"type":"output","id":"61i-Hvl2aKW8gAZaW48Td","data":[{"name":"stdout","output_type":"stream","text":"COMMON WORDS ANALYSIS\n"}],"key":"wGhkvN1A1f"}],"key":"ACoZb626qh"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# test it on 2024\nwords_2024 = get_most_common_words(sotu, 2024, n=10)\nprint(\"\\nTop 10 words for 2024:\")\nprint(words_2024)","key":"b4hMN5dxCn"},{"type":"output","id":"xTUq59P1xJJgzSoy8kPI6","data":[{"name":"stdout","output_type":"stream","text":"\nTop 10 words for 2024:\n[('president', 58), ('year', 45), ('america', 44), ('american', 34), ('people', 33), ('$', 33), ('member', 32), ('want', 29), ('audience', 29), ('know', 29)]\n"}],"key":"FmOLBKwzVA"}],"key":"ESYFkGEsqy"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":4,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Compare 2023 to 2017","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GNdCYuWXHe"}],"identifier":"compare-2023-to-2017","label":"Compare 2023 to 2017","html_id":"compare-2023-to-2017","implicit":true,"key":"luaBXqXpOj"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Run your function from the previous step to get the top 20 words for 2017 and 2023. Plot the words and their frequencies in a barchart and replicate the figure below.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"iO5HZltz5N"}],"key":"FPhg0eUwLH"}],"key":"mNmeScXCpY"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"words_2023 = get_most_common_words(sotu, 2023, n=20)\nwords_2017 = get_most_common_words(sotu, 2017, n=20)","key":"tVErFR2gVW"},{"type":"output","id":"l26495CPpZ7jkfEaHRAoc","data":[],"key":"fDAvF2T9xm"}],"key":"u6d1p6pDGW"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"words_2023","key":"XTmQeNF7dN"},{"type":"output","id":"xSXNLdogu8VBWgAAJAeLv","data":[{"output_type":"execute_result","execution_count":44,"metadata":{},"data":{"text/plain":{"content":"[('year', 58),\n ('go', 56),\n ('let', 45),\n ('know', 40),\n ('people', 39),\n ('job', 38),\n ('america', 36),\n ('come', 33),\n ('law', 33),\n ('pay', 33),\n ('american', 31),\n ('$', 31),\n ('president', 30),\n ('look', 27),\n ('world', 25),\n ('folk', 24),\n ('nation', 24),\n ('audience', 23),\n ('work', 23),\n ('right', 23)]","content_type":"text/plain"}}}],"key":"S6EB4JPeYa"}],"key":"GpBLJ8TjsO"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"words_2017","key":"gxdY05J3QG"},{"type":"output","id":"8Pg73yLQLh-nQHXQKd2ND","data":[{"output_type":"execute_result","execution_count":45,"metadata":{},"data":{"text/plain":{"content":"[('american', 34),\n ('america', 29),\n ('country', 26),\n ('nation', 21),\n ('great', 20),\n ('new', 19),\n ('year', 19),\n ('world', 18),\n ('job', 15),\n ('people', 15),\n ('americans', 14),\n ('united', 13),\n ('tonight', 13),\n ('states', 12),\n ('work', 12),\n ('child', 12),\n ('want', 12),\n ('time', 12),\n ('citizen', 11),\n ('right', 11)]","content_type":"text/plain"}}}],"key":"izNfu3c07y"}],"key":"OKoqCawMvt"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Hint - put the words and counts into a pd Dataframe for better structure\n# and to make plotting easier\ndf_2017 = pd.DataFrame(words_2017, columns=[\"Word\", \"Count\"])\ndf_2023 = pd.DataFrame(words_2023, columns=[\"Word\", \"Count\"])","key":"rJquv3u8t0"},{"type":"output","id":"Y41PZb5Crsf76W_51b0Bn","data":[],"key":"PsEH5xwbvR"}],"key":"HrhbVxZIb5"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Draw the bar charts\nfig, axes = plt.subplots(2, 1, figsize=(12, 8))\n\n# Chart for 2017\nsns.barplot(data=df_2017, x=\"Word\", y=\"Count\", ax=axes[0])\naxes[0].set_title(\"Most Frequent Terms in the 2017 Address\")\naxes[0].tick_params(axis=\"x\", rotation=45)\n\n# Chart for 2023\nsns.barplot(data=df_2023, x=\"Word\", y=\"Count\", ax=axes[1])\naxes[1].set_title(\"Most Frequent Terms in the 2023 Address\")\naxes[1].tick_params(axis=\"x\", rotation=45)\n\nplt.subplots_adjust(hspace=0.5)\nplt.tight_layout()\nplt.savefig(\"outputs/2.1_State_of_the_Union_Most_Frequent_Words.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n","key":"ZucjQ8vifL"},{"type":"output","id":"WM-RqvFVGsVVZHvWVyMxx","data":[{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"b5616c15a84e244056f3403909b8f92a","path":"/user/yau_christy/myst-build/proj02-group22/build/b5616c15a84e244056f3403909b8f92a.png"},"text/plain":{"content":"<Figure size 1200x800 with 2 Axes>","content_type":"text/plain"}}}],"key":"nvqvkDaJNT"}],"key":"GiEpENQ6Tf"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"TF-IDF Vectorization","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ixCVxfHabr"}],"identifier":"tf-idf-vectorization","label":"TF-IDF Vectorization","html_id":"tf-idf-vectorization","implicit":true,"key":"KJN6Np4Un3"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"To use statsitical alorithms on documents, we need to transform them into vectors, where each element of the vector corresponds to a particular word in a document or corpus of documents. One common way is via TF-IDF embeddings. LLMs work similarly - they typically use transformer models to generate text embeddings before sending text through a deep neural network.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"iK4MVLRZAc"}],"key":"vY1R2Z2Cwt"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Here we will fit a TF-IDF vectorizer, plot all the speeches on a 2-D grid using PCA and also using a heatmap, and examine TF-IDF scores for the top 10 most common words in the first speech. This is a good resource here: ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"Uy1nCvGzie"},{"type":"link","url":"https://medium.com/GeoffreyGordonAshbrook/vector-visualization-2d-plot-your-tf-idf-with-pca-83fa9fccb1d","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"https://​medium​.com​/GeoffreyGordonAshbrook​/vector​-visualization​-2d​-plot​-your​-tf​-idf​-with​-pca​-83fa9fccb1d","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"c7jqbdl9DV"}],"urlSource":"https://medium.com/GeoffreyGordonAshbrook/vector-visualization-2d-plot-your-tf-idf-with-pca-83fa9fccb1d","key":"VeRfhFETDP"}],"key":"OqOM1J8LeE"}],"key":"UgFzrVMnH5"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA","key":"c3PyF3toTA"},{"type":"output","id":"wgKgEzDldESDmv396WpIJ","data":[],"key":"Hd2EQ1pTZc"}],"key":"vKtcgFYB2x"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":4,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Train the Vectorizer and Transform the Data","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jYQlAbjxuM"}],"identifier":"train-the-vectorizer-and-transform-the-data","label":"Train the Vectorizer and Transform the Data","html_id":"train-the-vectorizer-and-transform-the-data","implicit":true,"key":"XrzObPEpA2"}],"key":"LklTXloVV8"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# you may use this as input to fit the TF-IDF vectorizer\nraw_docs = sou[\"Text\"].to_list()","key":"PFPHqO0HvN"},{"type":"output","id":"Im2aGYpyAsZBs-69bcAeu","data":[],"key":"Awb4CX0wtQ"}],"key":"QRJujUiDDq"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Hint - use fit_transform for vectorizer and PCA\n\n# Initialize TF-IDF vectorizer (default tokenizer is fine)\ntfidf_model = TfidfVectorizer()\n\n# Fit the model and transform the speeches into TF-IDF vectors\ntfidf_vectors = tfidf_model.fit_transform(raw_docs).toarray()\n\nprint(f\"TF-IDF matrix shape: {tfidf_vectors.shape}\")\n","key":"OKo0otORvF"},{"type":"output","id":"-4QP25X6f1Ub_fRK1W0Kb","data":[{"name":"stdout","output_type":"stream","text":"TF-IDF matrix shape: (246, 25957)\n"}],"key":"IIOfXPR6jX"}],"key":"uZzzABVKA0"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The output of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SqJvIiwySS"},{"type":"inlineCode","value":"fit_transform()","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jvafEaV80v"},{"type":"text","value":" will be a matrix where each row corresponds to a speech, each column corresponds to a word in the corpus of speeches, and the value is the TF-IDF score which measures the importance of that word in that speech, relative to the rest of the speeches.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RwXmev9hrG"}],"key":"Fg3WFI58ci"}],"key":"hdBGdg2DEw"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":4,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Plot Speeches","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hcvDjLbjjJ"}],"identifier":"plot-speeches","label":"Plot Speeches","html_id":"plot-speeches","implicit":true,"key":"hTe1fEyA5X"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"First used PCA to generate the first chart","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"i7Ap8JPFvY"}],"key":"Hh2o7tepyJ"}],"key":"EwvAK8zLpQ"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Second use seaborn heatmap with a log-scaled color axis to generate the second chart","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"kJPXwgpis1"}],"key":"kIFPyLwSyv"}],"key":"nidNJaARaL"}],"key":"UhbT4bbnFq"}],"key":"Z7GP51pXeF"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Step 1: Set PCA to find first 2 principal components\npca = PCA(n_components=2)\n\n# Step 2: Create a new dataframe where each row is a speech, and each column is a projection onto\n# one of the two principal components\ndf2d = pd.DataFrame(pca.fit_transform(tfidf_vectors), columns=[\"x\", \"y\"])\n\nplt.figure(figsize=(8, 6))\nplt.scatter(df2d[\"x\"], df2d[\"y\"], alpha=0.7)\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.title(\"PCA Plot of Vectorized Speeches (TF-IDF)\")\nplt.tight_layout()\nplt.savefig(\"outputs/vectorized_speeches_principal_components.png\")\nplt.show()","key":"DVdcocfwek"},{"type":"output","id":"WmTxryhUrb3pU_QESeh0H","data":[{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"b52eca35a6181c0ab134b55ba0b17f6c","path":"/user/yau_christy/myst-build/proj02-group22/build/b52eca35a6181c0ab134b55ba0b17f6c.png"},"text/plain":{"content":"<Figure size 800x600 with 1 Axes>","content_type":"text/plain"}}}],"key":"twIo3Qnoow"}],"key":"mhsec4H10F"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Hint - vectorized_docs is a sparse matrix whose rows are speeches and columns are tokens, with each\n# value being a TF-IDF score. Densify this array first, and then plot using seaborn.\n# TF-IDF HEATMAP WITH LOG-SCALED COLOR AXIS\nfrom matplotlib.colors import LogNorm\n\n# Convert sparse matrix → dense matrix\ndense_tfidf = tfidf_vectors  # Already dense if you used .toarray() earlier\n# If yours is sparse, uncomment:\n# dense_tfidf = tfidf_vectors.toarray()\n\nplt.figure(figsize=(7, 5))\n\nsns.heatmap(\n    dense_tfidf,\n    norm=LogNorm(),\n    cmap=\"magma\",\n)\n\nplt.title(\"Vectorized Speeches\")\nplt.xlabel(\"Vector Index\")\nplt.ylabel(\"Speech Index\")\n\nplt.tight_layout()\nplt.savefig(\"outputs/2.3_Vectorized_Speeches.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n","key":"mngvxIQYY2"},{"type":"output","id":"5BNDJVz8rq7yWMe62Q91A","data":[{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"7c6c3c79d863ba9f9ac71a0422e9d111","path":"/user/yau_christy/myst-build/proj02-group22/build/7c6c3c79d863ba9f9ac71a0422e9d111.png"},"text/plain":{"content":"<Figure size 700x500 with 2 Axes>","content_type":"text/plain"}}}],"key":"tnN0IZYfyD"}],"key":"CpN8P46h9G"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":4,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Get the TF-IDF value for certain words and documents","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LC3i9DsZB9"}],"identifier":"get-the-tf-idf-value-for-certain-words-and-documents","label":"Get the TF-IDF value for certain words and documents","html_id":"get-the-tf-idf-value-for-certain-words-and-documents","implicit":true,"key":"Iax3mipL39"}],"key":"vtadI4ngXT"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"word_list = ['year',\n 'america',\n 'people',\n 'american',\n 'work',\n 'new',\n 'job',\n 'country',\n 'americans',\n 'world'] # top ten most common words through whole corpus","key":"bH63jv9uPw"},{"type":"output","id":"mmkKRv0DKRFU9YktwKZkP","data":[],"key":"fw1u29qQDL"}],"key":"Cz7sJtmCnx"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"word_nums = tfidf_model.vocabulary_ # get each word's index number using the .vocabular_ attributed of vectorizer","key":"UHWeEtx5IQ"},{"type":"output","id":"6rzhtThtKg71uGPHPsxu_","data":[],"key":"WA5viMenG7"}],"key":"ypIdycr2WL"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"idf_score = []\nfor word in word_list:\n    idx = word_nums[word]\n    idf_score.append(tfidf_model.idf_[idx]) # get their IDF score by using .idf_ at the indices from the previous step","key":"jUDeGODDbc"},{"type":"output","id":"7JM0J2hr5kszsgT4wb4eg","data":[],"key":"iK2NN7JY4Q"}],"key":"LUF3KnabLq"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"tf_idf = []\nfor word in word_list:\n    idx = word_nums[word]\n    tf_idf.append(tfidf_vectors[0, idx])\n","key":"xms358iyo4"},{"type":"output","id":"4eP4g7imy6fuxIeZXdVhf","data":[],"key":"jFkjJw7ZmG"}],"key":"R4JBBsdh1t"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"pd.DataFrame({\"Word\": word_list, \"IDF Score\": idf_score, \"TF-IDF Score\": tf_idf})","key":"qHMhlLM1T2"},{"type":"output","id":"tK4XKq8JfG2Z5l-XdUWEW","data":[{"output_type":"execute_result","execution_count":57,"metadata":{},"data":{"text/html":{"content":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>IDF Score</th>\n      <th>TF-IDF Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>year</td>\n      <td>1.032925</td>\n      <td>0.022719</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>america</td>\n      <td>1.272946</td>\n      <td>0.068439</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>people</td>\n      <td>1.037118</td>\n      <td>0.043087</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>american</td>\n      <td>1.102217</td>\n      <td>0.045792</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>work</td>\n      <td>1.162281</td>\n      <td>0.005681</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>new</td>\n      <td>1.024591</td>\n      <td>0.016275</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>job</td>\n      <td>2.043480</td>\n      <td>0.009988</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>country</td>\n      <td>1.008130</td>\n      <td>0.013550</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>americans</td>\n      <td>1.713598</td>\n      <td>0.041877</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>world</td>\n      <td>1.138750</td>\n      <td>0.026438</td>\n    </tr>\n  </tbody>\n</table>\n</div>","content_type":"text/html"},"text/plain":{"content":"        Word  IDF Score  TF-IDF Score\n0       year   1.032925      0.022719\n1    america   1.272946      0.068439\n2     people   1.037118      0.043087\n3   american   1.102217      0.045792\n4       work   1.162281      0.005681\n5        new   1.024591      0.016275\n6        job   2.043480      0.009988\n7    country   1.008130      0.013550\n8  americans   1.713598      0.041877\n9      world   1.138750      0.026438","content_type":"text/plain"}}}],"key":"lyEgzSPzEy"}],"key":"YUh2OdB4fI"}],"key":"JG0uYwdj2g"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"Project 2: Reproducibility in Natural Language Processing","url":"/nlp-p01","group":"proj02"},"next":{"title":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","url":"/nlp-p03","group":"proj02"}}},"domain":"http://localhost:3000"}